{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Sample evaluation and cross validation\n",
    "\n",
    "\n",
    "It's come time to address another potential source of error in our models: overfitting. Overfitting is when your model is so excessively complex that it starts to catch random noise instead of describing the true underlying relationships. This is typically manifested with a model that evaluates as more accurate than it really is. In most situations you shouldn't be able to build a perfect model, some error is to be expected. Overfitting is extremely common and easy to do, but there are ways to guard against it. The main way is through how you evaluate your model.\n",
    "\n",
    "Thus far we've been using our training data to evaluate our model. By this we mean that we've used the same data to train the model and to see how well the model is doing. When you think about it, some of the danger of that approach may become apparent. If we create a very elaborate model it will pick up on the nuances of the data that are just from random noise. If we evaluate the model on the training data, that ability to pick up noise will be returned as accuracy. In reality, this isn't the case and doesn't depict how we'd really want to evaluate a model. Generally we don't care about predicting things we already know. We care about other data, new information, or other situations. This is why testing with training data really isn't what we want to do.\n",
    "\n",
    "But if that's the case, what can we do?\n",
    "\n",
    "Holdout Groups\n",
    "The simplest way to combat overfitting is with a holdout group (or sometimes \"holdback group\"). All this means is that you do not include all of your data in your training set, instead reserving some of it exclusively for testing. While there is a cost to having less training data, your evaluation will be far more reliable.\n",
    "\n",
    "When directly comparing two models that are based on different techniques or different specifications, this holdout method combats overfitting. Overfit models will see a drop in success rate outside of their training data, and so their performance will not be artificially inflated as it would be if you trained and validated your model using the whole data set. This is because they got really good at matching the patterns within the data they were trained with, but didn't actually learn the things that matter but random noise. When they try to match that random noise on new data their accuracy suffers.\n",
    "\n",
    "How much data you choose to keep in a holdout is really up to you and depends on how much and what kind of data you have to begin with as well as what kind of model you're training. You should check and see how much variance your model has as you add more data as well as how much data it would take to maintain a reasonably representative test sample. It is, however, a balance. 30% is a common starting point, but really anything from 50% to 1% of the original dataset could be reasonable.\n",
    "\n",
    "This seems relatively simple to code up. We'll try it below with our spam model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab and process the raw data.\n",
    "data_path = (\"https://raw.githubusercontent.com/Thinkful-Ed/data-201-resources/\"\n",
    "             \"master/sms_spam_collection/SMSSpamCollection\"\n",
    "            )\n",
    "sms_raw = pd.read_csv(data_path, delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['spam', 'message']\n",
    "\n",
    "# Enumerate our spammy keywords.\n",
    "keywords = ['click', 'offer', 'winner', 'buy', 'free', 'cash', 'urgent']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.message.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "\n",
    "sms_raw['allcaps'] = sms_raw.message.str.isupper()\n",
    "sms_raw['spam'] = (sms_raw['spam'] == 'spam')\n",
    "data = sms_raw[keywords + ['allcaps']]\n",
    "target = sms_raw['spam']\n",
    "\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "y_pred = bnb.fit(data, target).predict(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.884304932735426\n",
      "Testing on Sample: 0.8916008614501076\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These scores look really consistent! It doesn't seem like our model is overfitting. Part of the reason for that is that it's so simple (more on that in a bit). But we should look and see if any other issues are lurking here. So let's try a more robust evaluation technique, cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross Validation\n",
    "Cross validation is a more robust version of holdout groups. Instead of creating just one holdout, you create several.\n",
    "\n",
    "The way it works is this: start by breaking up your data into several equally sized pieces, or folds. Let's say you make x_ folds. You then go through the training and testing process _x times, each time with a different fold held out from the training data and used as the test set. The number of folds you create is up to you, but it will depend on how much data you want in your testing set. At its most extreme, you're creating the same number of folds as you have observations in your data set. This kind of cross validation has a special name: Leave One Out. Leave one out is useful if you're worried about single observations skewing your model, whereas large folds combat more general overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89784946, 0.89426523, 0.89426523, 0.890681  , 0.89605735,\n",
       "       0.89048474, 0.88150808, 0.89028777, 0.88489209, 0.89568345])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly what we'd hope to see. The array that cross_val_score returns is a series of accuracy scores with a different hold out group each time. If our model is overfitting at a variable amount, those scores will fluctuate. Instead, ours are relatively consistent.\n",
    "\n",
    "Above we used the SKLearn built in functions for both of these kinds of cross validation, the documentation for which can be found here. However, the outputs from that are somewhat limited. By default it uses the score method. You can adjust what is returned, but you don't get all of the error types or outputs you may be interested in. That's why it's not uncommon for people to code up their own cross validation.\n",
    "\n",
    "To make sure you understand how cross validation works, try to code it up yourself below, not relying on SKLearn:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your own cross validation with your spam model.\n",
    "#data = sms_raw[[keywords] + 'allcaps']\n",
    "\n",
    "#target = the boolean values in the spam column. These are the actual values\n",
    "\n",
    "#for each row of the message variable.\n",
    "\n",
    "#bnb this tells the computer that the model is bernoulian. Which is to\n",
    "\n",
    "#say, it will only have two variables\n",
    "\n",
    "#what I need: test_1, test_2, test_3, test_4, test_5\n",
    "\n",
    "sample_1, sample_2, sample_3, sample_4, sample_5\n",
    "cross_val_score(bnb, data, target, cv=10)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20) print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test))) print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))\n",
    "\n",
    "print(sms_raw)\n",
    "\n",
    "target_1 = sms_raw.loc[:1] sbnb.fit((data, sms_raw['spam'][] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 10)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1114.4"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5572 * .20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       False\n",
      "1       False\n",
      "2        True\n",
      "3       False\n",
      "4       False\n",
      "5        True\n",
      "6       False\n",
      "7       False\n",
      "8        True\n",
      "9        True\n",
      "10      False\n",
      "11       True\n",
      "12       True\n",
      "13      False\n",
      "14      False\n",
      "15       True\n",
      "16      False\n",
      "17      False\n",
      "18      False\n",
      "19       True\n",
      "20      False\n",
      "21      False\n",
      "22      False\n",
      "23      False\n",
      "24      False\n",
      "25      False\n",
      "26      False\n",
      "27      False\n",
      "28      False\n",
      "29      False\n",
      "        ...  \n",
      "1084    False\n",
      "1085    False\n",
      "1086    False\n",
      "1087    False\n",
      "1088    False\n",
      "1089     True\n",
      "1090    False\n",
      "1091     True\n",
      "1092    False\n",
      "1093    False\n",
      "1094    False\n",
      "1095    False\n",
      "1096    False\n",
      "1097     True\n",
      "1098    False\n",
      "1099    False\n",
      "1100    False\n",
      "1101    False\n",
      "1102    False\n",
      "1103    False\n",
      "1104    False\n",
      "1105     True\n",
      "1106    False\n",
      "1107    False\n",
      "1108    False\n",
      "1109    False\n",
      "1110    False\n",
      "1111    False\n",
      "1112    False\n",
      "1113    False\n",
      "Name: spam, Length: 1114, dtype: bool\n"
     ]
    }
   ],
   "source": [
    "target_1 = sms_raw.iloc[0:1114, 0]\n",
    "print(target_1)\n",
    "\n",
    "target_2 = sms_raw.iloc[1114:2228, 0]\n",
    "\n",
    "target_3 = sms_raw.iloc[2228:3342, 0]\n",
    "\n",
    "target_4 = sms_raw.iloc[3342:4456, 0]\n",
    "\n",
    "target_5 = sms_raw.iloc[4456:5570, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      click  offer  winner    buy   free   cash  urgent  allcaps\n",
      "0     False  False   False  False  False  False   False    False\n",
      "1     False  False   False  False  False  False   False    False\n",
      "2     False  False   False  False  False  False   False    False\n",
      "3     False  False   False  False  False  False   False    False\n",
      "4     False  False   False  False  False  False   False    False\n",
      "5     False  False   False  False  False  False   False    False\n",
      "6     False  False   False  False  False  False   False    False\n",
      "7     False  False   False  False  False  False   False    False\n",
      "8     False  False   False  False  False  False   False    False\n",
      "9     False  False   False  False   True  False   False    False\n",
      "10    False  False   False  False  False  False   False    False\n",
      "11    False  False   False  False  False  False   False    False\n",
      "12    False  False   False  False   True  False   False    False\n",
      "13    False  False   False  False  False  False   False    False\n",
      "14    False  False   False  False  False  False   False     True\n",
      "15     True  False   False  False  False  False   False    False\n",
      "16    False  False   False  False  False  False   False    False\n",
      "17    False  False   False  False  False  False   False    False\n",
      "18    False  False   False  False  False  False   False    False\n",
      "19    False  False   False  False  False  False   False    False\n",
      "20    False  False   False  False  False  False   False    False\n",
      "21    False  False   False  False  False  False   False    False\n",
      "22    False  False   False  False  False  False   False    False\n",
      "23    False  False   False  False  False  False   False    False\n",
      "24    False  False   False  False  False  False   False    False\n",
      "25    False  False   False  False  False  False   False    False\n",
      "26    False  False   False  False  False  False   False    False\n",
      "27    False  False   False  False  False  False   False    False\n",
      "28    False  False   False  False  False  False   False    False\n",
      "29    False  False   False  False  False  False   False    False\n",
      "...     ...    ...     ...    ...    ...    ...     ...      ...\n",
      "1084  False  False   False  False  False  False   False    False\n",
      "1085  False  False   False  False  False  False   False    False\n",
      "1086  False  False   False  False  False  False   False    False\n",
      "1087  False  False   False  False  False  False   False    False\n",
      "1088  False  False   False  False  False  False   False    False\n",
      "1089  False  False   False  False  False  False   False    False\n",
      "1090  False  False   False  False  False  False   False    False\n",
      "1091  False  False   False  False  False  False   False    False\n",
      "1092  False  False   False  False  False  False   False    False\n",
      "1093  False  False   False  False  False  False   False    False\n",
      "1094  False  False   False  False  False  False   False    False\n",
      "1095  False  False   False  False  False  False   False    False\n",
      "1096  False  False   False  False  False  False   False    False\n",
      "1097  False  False   False  False  False  False   False    False\n",
      "1098  False  False   False   True  False  False   False    False\n",
      "1099  False  False   False  False  False  False   False    False\n",
      "1100  False  False   False  False  False  False   False    False\n",
      "1101  False  False   False  False  False  False   False    False\n",
      "1102  False  False   False  False  False  False   False    False\n",
      "1103  False  False   False  False  False  False   False    False\n",
      "1104  False  False   False  False  False  False   False    False\n",
      "1105  False  False   False  False  False  False   False    False\n",
      "1106  False  False   False  False  False  False   False    False\n",
      "1107  False  False   False  False  False  False   False    False\n",
      "1108  False  False   False  False  False  False   False    False\n",
      "1109  False  False   False  False  False  False   False    False\n",
      "1110  False  False   False  False  False  False   False    False\n",
      "1111  False  False   False  False  False  False   False    False\n",
      "1112  False  False   False  False  False  False   False    False\n",
      "1113  False  False   False  False  False  False   False    False\n",
      "\n",
      "[1114 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "data_1 = sms_raw.iloc[0:1114, 2:11]\n",
    "print(data_1)\n",
    "\n",
    "data_2 = sms_raw.iloc[1114:2228, 2:11]\n",
    "\n",
    "data_3 = sms_raw.iloc[2228:3342, 2:11]\n",
    "\n",
    "data_4 = sms_raw.iloc[3342:4456, 2:11]\n",
    "\n",
    "data_5 = sms_raw.iloc[4456:5570, 2:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8824057450628366"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation1_20percent = bnb.fit(data_1, target_1).score(data_1, target_1)\n",
    "\n",
    "validation1_20percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9012567324955116"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation2_20percent = bnb.fit(data_2, target_2).score(data_2, target_2)\n",
    "\n",
    "validation2_20percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.900359066427289"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation3_20percent = bnb.fit(data_3, target_3).score(data_3, target_3)\n",
    "\n",
    "validation3_20percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8842010771992819"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation4_20percent= bnb.fit(data_4, target_4).score(data_4, target_4)\n",
    "\n",
    "validation4_20percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8922800718132855"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation5_20percent = bnb.fit(data_5, target_5).score(data_5, target_5)\n",
    "\n",
    "validation5_20percent  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's a good score?\n",
    "When we're looking at this model, we've been getting accuracy scores around .89. Intuitively that seems like a pretty good score, but in the start of this lesson we mentioned different kinds of error. We also mentioned class imbalance. Both of these things are at play here. Using the topics we introduced earlier in this lesson, try to do a more in depth evaluation of the model looking at the kind of errors we're generating and what accuracy we'd get if we just randomly guessed. You may want to use what's known as a confusion matrix to show different kinds of errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred : [False False False ... False False False]\n",
      "total rows/messages = 5572\n",
      "True positive:  198\n",
      "True negative;  4770\n",
      "False positive:  55\n",
      "False negative:  549\n"
     ]
    }
   ],
   "source": [
    "# Perform your additional evaluation here.\n",
    "# Build your confusion matrix and calculate sensitivity and specificity here.\n",
    "\n",
    "print('y_pred :', y_pred)\n",
    "\n",
    "print('total rows/messages =', data.shape[0])\n",
    "\n",
    "#True positive:\n",
    "print('True positive: ', ((target == True) & (y_pred == True)).sum())\n",
    "\n",
    "#True negative\n",
    "print('True negative; ', ((target == False) & (y_pred == False)).sum())\n",
    "\n",
    "#False positive\n",
    "print('False positive: ', ((target == False) & (y_pred == True)).sum())\n",
    "\n",
    "#False negative\n",
    "print('False negative: ', ((target == True) & (y_pred == False)).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thinking like a Data Scientist\n",
    "\n",
    "How you choose to validate your model in real life will depend upon the kind of data you're working with and the kinds of concerns you have about the model's performance. Remember, your model is trained to fit the data you feed it, so if the situation changes your model will become less accurate. For example, if there are seasonal changes to your observed variable but you only train on one month's data, you're going to have a problem. You could test that by seeing how accurate your model is with a specific time period as your holdout, rather than a random sample. We'll cover techniques for dealing with time more later.\n",
    "\n",
    "Overfitting and Naive Bayes\n",
    "Overfitting is always possible, but some models are more susceptible to it than others. Naive Bayes is actually pretty good for avoiding overfitting. This is largely because the assumptions are so simple, particularly the assumed independence between any two independent variables. One of the sources of overfitting is when a model tries to map complex interactions between variables that aren't really there or significant. Naive Bayes cannot do this because it assumes they are all independent and therefore not interacting. It's a nice characteristic at times, but it does mean it doesn't take into account how your features affect each other.\n",
    "\n",
    "Also, one final note on our models here. They weren't overfitting, but they weren't telling us much either. They were just barely more accurate than the dominant class. Discuss with your mentor why that is and what you could do to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discuss with Mike\n",
    "\n",
    "Also, one final note on our models here. They weren't overfitting, but they weren't telling us much either. They were just barely more accurate than the dominant class. Discuss with your mentor why that is and what you could do to improve the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
